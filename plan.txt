imports

Agent 
    /__init()
        
        model=
        target_model=
    /select Action # e greedy method 
    
    /train 

    /test 
    
    /target update
    y_Double= reward' + gamma* target_predict(S')[argmax_main(S')]
    

    class Network(keras.Model):
    def __init__(self, num_actions : int, dueling: bool):
        super(Network, self).__init__()
        self.dueling = dueling
        self.dense1 = layer.Dense(24, activation='relu')
        self.dense2 = layer.Dense(24, activation='relu')
        self.adv_dense = layer.Dense(24, activation='relu')
        self.adv_out = layer.Dense(num_actions, activation='relu')

        if self.dueling:
            self.v_dense = layer.Dense(24, activation='relu')
            self.v_out = layer.Dense(1, activation='relu')
            self.lambda_layer = layer.Lambda(lambda x: x - tf.reduce_mean(x))
            self.combine = layer.Add()

    def call(self, input):
        x = self.dense1(input)
        x = self.dense2(x)
        adv_1 = self.adv_dense(x)
        adv = self.adv_out(adv_1)
        if self.dueling:
            # v = self.v_dense(x)
            v = self.v_out(adv_1)
            norm_adv = self.lambda_layer(adv)
            combined = self.combine([v, norm_adv])
            return combined
        return adv

    class Net(nn.Module):
    def __init__(self,n_in,n_mid,n_out):
        super(Net,self).__init__()
        self.fc1 = nn.Linear(n_in,n_mid)
        self.fc2 = nn.Linear(n_mid,n_mid)
        
        # Dueling 
        self.fc3_adv = nn.Linear(n_mid,n_out) # advantage 신경망 
        self.fc3_v = nn.Linear(n_mid,1) # value 
    
    def forward(self,x):
        h1 = F.relu(self.fc1(x))
        h2 = F.relu(self.fc2(h1))
        
        adv = self.fc3_adv(h2)
        val = self.fc3_v(h2).expand(-1,adv.size(1))
        
        output = val + adv - adv.mean(1,keepdim=True).expand(-1,adv.size(1))
        return output 
   
    /Networks build 
        init:
            input(state_size, 24, activation='relu')
            은닉층(24,relu)
            output(action_size,'linear')
            compile(loss=mse,optimizer=adam)
            
    /replay buffer
        init

        store

        sampling #random 

main()
    env=gym.make('cartpole-v1')

    agent=Agent()
    
    state,action_size
    for e in range(500) # num_epi 
        done=False
        episod시작
        while not done:
            action select
            observation, reward, step,done 진행하고

            set S_t+1
            store transition(S,A,R,S')
            if len(replay_buffer) > 3000: buffer max size min
                minibatch sampling()
                gradient descent step

            